{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs\n",
    "### W08D4 NLP II\n",
    "Instructor: Socorro Dominguez  \n",
    "February 25, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda:**\n",
    "* Introduction to NLP modeling\n",
    "\n",
    "* Sentiment analysis\n",
    "    * Supervised learning sentiment analysis\n",
    "\n",
    "* Topic modeling\n",
    "    * LDA (Latent-Dirichlet-Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim \n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim import matutils, models\n",
    "import pyLDAvis.gensim\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment Anaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Supervised Learning Algorithms for Sentiment Analysis\n",
    "\n",
    "Naive Bayes is popular in text classification tasks. \n",
    "\n",
    "You have used NB before. Today, we will use it for sentiment analysis, which is a problem of assigning positive or negative label to a text based on the sentiment or attitude expressed in  it. \n",
    "\n",
    "For this example, we will use [IMDB movie review data set](https://www.kaggle.com/utathya/imdb-review-dataset). If you want to reproduce this example, you will need to download the data on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading data and preprocessing\n",
    "\n",
    "1. We need to load data CSV as a pandas DataFrame.\n",
    "\n",
    "2. There are three possible labels in the dataset: `pos`, `neg`, and `unsup`. For now, let's discard rows with `unsup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('data/imdb_master.csv', encoding = \"ISO-8859-1\")\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unsup    50000\n",
       "neg      25000\n",
       "pos      25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# only consider positive and negative reviews\n",
    "imdb_df = imdb_df[imdb_df['label'].str.startswith(('pos','neg'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature extraction\n",
    "\n",
    "The current data is in the form of moview reviews (text paragraphs) and their targets (`pos` or `neg`). \n",
    "We need to encode movie reviews into feature vectors so that we can train supervised machine learning models with `scikit-learn`. \n",
    "\n",
    "How can we do this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create word frequency counts (`X_counts`)\n",
    "Turn the text into sparse vector of word frequency counts using [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from  `scikit-learn`. \n",
    "\n",
    "When you reproduce this, explore the arguments of `CountVectorizer` (e.g., [`stop_words`](https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words), `ngram_range`, `max_features`, `min_df`, and `tokenizer`).  \n",
    "\n",
    "#### Create binarized representation of words (`X_binary`)\n",
    "Create binarized encoding (`X_binary`) of `X_counts`, where you replace word frequencies $\\geq$ 1 by 1.    \n",
    "The intuition behind using binarized representation is that for sentiment analysis word occurrence may matter more than word frequency. For instance, the occurrence of the word _excellent_ tells us a lot and the fact that it occurs four times may not tell us much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seiryu8808/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/Users/seiryu8808/opt/anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "import nltk\n",
    "# For converting words into frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize movie_vector object and then turn movie reviews train data into a vector \n",
    "movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, stop_words='english')\n",
    "\n",
    "# use top 5000 words only\n",
    "# movie_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features = 5000) \n",
    "X_counts = movie_vec.fit_transform(imdb_df['review'])\n",
    "\n",
    "# Convert raw frequency counts into binarized representation. \n",
    "X_binary = X_counts > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train Naive Bayes classifier\n",
    "\n",
    "1. Split (`X_counts`, `imdb_df.label`) into train (80%) and test (20%).\n",
    "2. Train [multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) on the train set. \n",
    "3. Report train and test accuracies.\n",
    "4. Now repeat steps 1, 2, and 3 with (`X_binary`, `imdb_df.label`). \n",
    "5. Compare your results for `X_counts` and `X_binary` and note your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_NB_train_test_accuracies(X, y, classifier = 'multinominal'):\n",
    "    \"\"\"\n",
    "    Given X, y, and the classifier, this function splits the \n",
    "    data into train and test splits, prints the train and test accuracies,\n",
    "    and returns the model.     \n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.20, \n",
    "                                                        random_state = 12)\n",
    "    if classifier.startswith('multinominal'):\n",
    "        model = MultinomialNB().fit(X_train, y_train)\n",
    "    elif classifier.startswith('bernoulli'):\n",
    "        model = BernoulliNB().fit(X_train, y_train)\n",
    "    print('Training accuracy:', model.score(X_train, y_train))\n",
    "    print('Test accuracy: ', model.score(X_test, y_test))\n",
    "    print('---------')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on binarized encoding \n",
      "Training accuracy: 0.90135\n",
      "Test accuracy:  0.8567\n",
      "---------\n",
      "Evaluation on counts encoding \n",
      "Training accuracy: 0.89905\n",
      "Test accuracy:  0.8558\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation on binarized encoding ')\n",
    "model_binary = get_NB_train_test_accuracies(X_binary, imdb_df.label, classifier = 'bernoulli')\n",
    "\n",
    "print('Evaluation on counts encoding ')\n",
    "model_counts = get_NB_train_test_accuracies(X_counts, imdb_df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's play with fake reviews \n",
    "\n",
    "Let's see how the model performs on fake movie reviews. Some examples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fake_reviews = ['This movie was excellent! The performances were oscar-worthy!',\n",
    "               'Unbelievably disappointing.', \n",
    "               'Full of zany characters and richly applied satire, and some great plot twists',\n",
    "               'This is the greatest screwball comedy ever filmed',\n",
    "               'It was pathetic. The worst part about it was the boxing scenes.', \n",
    "               '''It could have been a great movie. It could have been excellent, \n",
    "                and to all the people who have forgotten about the older, \n",
    "                greater movies before it, will think that as well. \n",
    "                It does have beautiful scenery, some of the best since Lord of the Rings. \n",
    "                The acting is well done, and I really liked the son of the leader of the Samurai.\n",
    "                He was a likeable chap, and I hated to see him die...\n",
    "                But, other than all that, this movie is nothing more than hidden rip-offs.\n",
    "                '''\n",
    "              ]\n",
    "gold_labels = ['pos', 'neg', 'pos', 'pos', 'neg', 'neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create word count encoding of the reviews.  \n",
    "fake_reviews_counts = movie_vec.transform(fake_reviews)\n",
    "fake_reviews_binary = fake_reviews_counts > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Predict using the Naive Bayes classifier\n",
    "predictions = model_binary.predict(fake_reviews_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'neg', 'pos', 'pos', 'neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(predictions.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Gold labels</th>\n",
       "      <th>NB labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was excellent! The performances were oscar-worthy!</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unbelievably disappointing.</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full of zany characters and richly applied satire, and some great plot twists</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the greatest screwball comedy ever filmed</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was pathetic. The worst part about it was the boxing scenes.</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It could have been a great movie. It could have been excellent, \\n                and to all the people who have forgotten about the older, \\n                greater movies before it, will think that as well. \\n                It does have beautiful scenery, some of the best since Lord of the Rings. \\n                The acting is well done, and I really liked the son of the leader of the Samurai.\\n                He was a likeable chap, and I hated to see him die...\\n                But, other than all that, this movie is nothing more than hidden rip-offs.\\n</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Review  \\\n",
       "0  This movie was excellent! The performances were oscar-worthy!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "1  Unbelievably disappointing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "2  Full of zany characters and richly applied satire, and some great plot twists                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3  This is the greatest screwball comedy ever filmed                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4  It was pathetic. The worst part about it was the boxing scenes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "5  It could have been a great movie. It could have been excellent, \\n                and to all the people who have forgotten about the older, \\n                greater movies before it, will think that as well. \\n                It does have beautiful scenery, some of the best since Lord of the Rings. \\n                The acting is well done, and I really liked the son of the leader of the Samurai.\\n                He was a likeable chap, and I hated to see him die...\\n                But, other than all that, this movie is nothing more than hidden rip-offs.\\n                   \n",
       "\n",
       "  Gold labels NB labels  \n",
       "0  pos         pos       \n",
       "1  neg         neg       \n",
       "2  pos         pos       \n",
       "3  pos         pos       \n",
       "4  neg         neg       \n",
       "5  neg         pos       "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "d = {'Review':fake_reviews, 'Gold labels':gold_labels, 'NB labels':predictions}\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Our model works well when there are clear words indicating whether the review is positive or negative, as the features we are using are word features.\n",
    "2. Fails for more complex examples, where understanding the context and overall text is essential to correctly classify reviews. The last example has many positive words in the beginning but the last sentence negates all positivity in the previous text. We need to incorporate deeper linguistic knowledge to correctly classify such cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/seiryu8808/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "VADER's SentimentIntensityAnalyzer() takes in a string and returns a dictionary of scores in each of four categories:\n",
    "\n",
    "* negative\n",
    "* neutral\n",
    "* positive\n",
    "* compound (computed by normalizing the scores above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.412, 'neu': 0.588, 'pos': 0.0, 'compound': -0.6818}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'The weather today is horrible. I dont feel like getting out'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.477, 'neu': 0.523, 'pos': 0.0, 'compound': -0.8074}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'This was the worst film to ever disgrace the screen.'\n",
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Gold labels</th>\n",
       "      <th>NB labels</th>\n",
       "      <th>Vader labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie was excellent! The performances were oscar-worthy!</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unbelievably disappointing.</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Full of zany characters and richly applied satire, and some great plot twists</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the greatest screwball comedy ever filmed</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was pathetic. The worst part about it was the boxing scenes.</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          Review  \\\n",
       "0  This movie was excellent! The performances were oscar-worthy!                   \n",
       "1  Unbelievably disappointing.                                                     \n",
       "2  Full of zany characters and richly applied satire, and some great plot twists   \n",
       "3  This is the greatest screwball comedy ever filmed                               \n",
       "4  It was pathetic. The worst part about it was the boxing scenes.                 \n",
       "\n",
       "  Gold labels NB labels Vader labels  \n",
       "0  pos         pos       pos          \n",
       "1  neg         neg       neg          \n",
       "2  pos         pos       pos          \n",
       "3  pos         pos       pos          \n",
       "4  neg         neg       neg          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Vader_scores'] = df['Review'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "df['compound']  = df['Vader_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df['Vader labels'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "\n",
    "df_labels = df[['Review', 'Gold labels', 'NB labels', 'Vader labels']]\n",
    "\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "10 min Break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling \n",
    "\n",
    "- Suppose your company has a large collection of documents on a variety of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of food magazines \n",
    "<center>\n",
    "<img src=\"images/00_TM_food_magazines.png\" height=\"2000\" width=\"2000\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: A corpus of news articles \n",
    "<center>\n",
    "<img src=\"images/01_TM_NYT_articles.png\" height=\"2000\" width=\"2000\"> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling \n",
    "\n",
    "- Suppose your company has a large collection of documents on a variety of topics\n",
    "- Suppose they ask you to \n",
    "    - infer different topics in the documents\n",
    "    - pull all documents about a certain topic    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- Humans are pretty good at reading and understanding documents and answering questions such as \n",
    "    - What is it about?  \n",
    "    - What is it related to in terms of content?     \n",
    "- Labeling by hand? \n",
    "    - Probably not\n",
    "- Use topic modeling which automates this process of inferring underlying structure in a large corpus of text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: Input \n",
    "\n",
    "<center>\n",
    "<img src=\"images/02_TM_science_articles.png\" height=\"2000\" width=\"2000\"> \n",
    "</center>\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output\n",
    "<center>\n",
    "<img src=\"files/images/TM_topics.png\" height=\"900\" width=\"900\"> \n",
    "</center>\n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling: output with interpretation\n",
    "\n",
    "- The labels are assigned manually.  \n",
    "<center>\n",
    "<img src=\"images/03_TM_topics_with_labels.png\" height=\"800\" width=\"800\"> \n",
    "</center>\n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Topic modeling pipeline \n",
    "\n",
    "- Feed knowlege into the machines; let it read large amount of text\n",
    "    * E.g., Wikipedia or News articles     \n",
    "- Preprocess your corpus \n",
    "    - Be careful with the features (i.e., words)\n",
    "- Train ML models\n",
    "    - For now Latent Dirichlet Allocation (LDA)\n",
    "- Interpret your topics     \n",
    "- Evaluate\n",
    "    - How well your model does on unseen documents? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baysian approach: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- Developed by [David Blei](http://www.cs.columbia.edu/~blei/) and colleagues. \n",
    "    * One of the most cited papers in the last 15 years.\n",
    "    \n",
    "- Insight: \n",
    "    - Each document is a random mixture of corpus-wide topics\n",
    "        - Every document is a discrete probability distribution of topics\n",
    "\n",
    "    - Every topic is a mixture words\n",
    "        - Every topic is a discrete probability distribution of words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA: insight\n",
    "- Each document is a random mixture of corpus-wide topics\n",
    "- Every topic is a mixture words\n",
    "<center>\n",
    "<img src=\"images/04_TM_dist_topics_words_blei.png\" height=\"1000\" width=\"1000\"> \n",
    "</center>\n",
    "\n",
    "(Credit: [Dave Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Every document is a discrete probability distribution of topics\n",
    "\n",
    "- Assume two topics: Topic 1 (topic model) and Topic 2 (fashion model)\n",
    "- Document 1: 100% topic models\n",
    "- Document 4: 100% fashion models\n",
    "- Document 7: 60% topic models + 40% fashion model\n",
    "\n",
    "<blockquote>\n",
    "Document 1: probabilistic topic model<br>\n",
    "Document 2: probabilistic topic model<br>\n",
    "Document 3: probabilistic topic model<br>\n",
    "Document 4: famous fashion model<br>\n",
    "Document 5: famous fashion model<br>\n",
    "Document 6: famous fashion model<br>\n",
    "Document 7: famous fashion model at probabilistic topic model conference<br>    \n",
    "</blockquote>\n",
    "    \n",
    "(Credit: The example is adapted from [Topic models tutorial](http://topicmodels.info/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Every topic is a discrete probability distribution of words\n",
    "\n",
    "- Assume two topics: Topic 1 (topic model) and Topic 2 (fashion model)\n",
    "- Topic 1: _model_ (0.33), _probabilistic_ (0.32), _topic_ (0.32), ...    \n",
    "- Topic 2: _model_ (0.33), _famous_ (0.32), _fashion_ (0.32), ...    \n",
    "\n",
    "<blockquote>\n",
    "Document 1: probabilistic topic model<br>\n",
    "Document 2: probabilistic topic model<br>\n",
    "Document 3: probabilistic topic model<br>\n",
    "Document 4: famous fashion model<br>\n",
    "Document 5: famous fashion model<br>\n",
    "Document 6: famous fashion model<br>\n",
    "Document 7: famous fashion model at probabilistic topic model conference<br>    \n",
    "</blockquote>\n",
    "    \n",
    "(Credit: The example is adapted from [Topic models tutorial](http://topicmodels.info/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intuition\n",
    "\n",
    "What is Dirichlet???\n",
    "\n",
    "What are our objectives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/theparty.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/dangers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/DirichletDistributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA model\n",
    "\n",
    "- Observable features: words\n",
    "- All other parameters are hidden or latent\n",
    "\n",
    "<center>\n",
    "<img src=\"images/05_TM_topic_model_blei.png\" height=\"700\" width=\"700\"> \n",
    "</center>\n",
    "\n",
    "(Adapted from [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA Machine\n",
    "\n",
    "- We want to get the best settings\n",
    "![img](images/Lda_machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/words_triangle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/topics_triangle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](images/2distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA: Hyperparameters\n",
    "\n",
    "- $\\alpha$ \n",
    "   - High alpha &rarr; every document contains a mixture of most of the topics\n",
    "   - Low alpha &rarr; every document is representative of only a few topic\n",
    "- $\\beta$\n",
    "    - High beta &rarr; Every topic contains a mixture of most of the words\n",
    "    - Low beta &rarr; Every topic contains a mixture of only few words\n",
    "\n",
    "<center>\n",
    "<img src=\"images/05_TM_topic_model_blei.png\" height=\"600\" width=\"600\"> \n",
    "</center>\n",
    "\n",
    "(Adapted from [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/BleiNgJordan2003.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/probas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/the_blueprint_relationship.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA learning: goals\n",
    "\n",
    "Infer the underlying topic structure in the documents. In particular, \n",
    "- Learn the probability distribution of topics in each document\n",
    "- Learn the discrete probability distribution of words in each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA learning: intuition\n",
    "\n",
    "Intuition: A word in a document is likely to belong to the same topic as the other words in that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm \n",
    "\n",
    "- Choose the number of topics you think are there in your corpus\n",
    "    * Example: k = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA algorithm\n",
    "\n",
    "- Repeat the following steps till the topics make sense:     \n",
    "- Randomly assign each words in each document to one of the topics\n",
    "    * Example: The word _probabilistic_ is randomly assigned to topic 2 (fashion).\n",
    "- Go through every word and its topic assignment in each document, looking at\n",
    "    * How often the topic occurs in the document?\n",
    "    * How often the word occurs with the topic overall? \n",
    "    * Example: Seems like topic 2 does not occur in Document 1 and the word _probabilistic_ doesn't occur much in topic 2 (fashion). So the word _probabilistic_ should probably be assigned to topic 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training LDA with [Gensim](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "You need\n",
    "\n",
    "- Document-term matrix \n",
    "- Pick number of topics: `num_topics`\n",
    "- Pick number of passes: `passes`\n",
    "\n",
    "\n",
    "\n",
    "* *Disclaimer: You can also check out Sklearn's model. However, Gensim is more used in NLP.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                       text\n",
       "0   1       famous fashion model     \n",
       "1   2       famous fashion model     \n",
       "2   3       famous fashion model     \n",
       "3   4       famous fashion model     \n",
       "4   5       famous fashion model     \n",
       "5   6       famous fashion model     \n",
       "6   7       famous fashion model     \n",
       "7   8       famous fashion model     \n",
       "8   9       famous fashion model     \n",
       "9   10      famous fashion model     \n",
       "10  11      famous fashion model     \n",
       "11  12      famous fashion model     \n",
       "12  13      probabilistic topic model\n",
       "13  14      probabilistic topic model\n",
       "14  15      probabilistic topic model\n",
       "15  16      probabilistic topic model\n",
       "16  17      probabilistic topic model\n",
       "17  18      probabilistic topic model\n",
       "18  19      probabilistic topic model\n",
       "19  20      probabilistic topic model\n",
       "20  21      probabilistic topic model\n",
       "21  22      probabilistic topic model\n",
       "22  23      probabilistic topic model\n",
       "23  24      probabilistic topic model\n",
       "24  25      probabilistic topic model\n",
       "25  26      probabilistic topic model"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv('data/toy_lda_data.csv')\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['famous', 'fashion', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model'],\n",
       " ['probabilistic', 'topic', 'model']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [doc.split() for doc in toy_df['text'].tolist()]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)],\n",
       " [(2, 1), (3, 1), (4, 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vocabulary for the lda model and \n",
    "# convert our corpus into document-term matrix for Lda\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=doc_term_matrix, \n",
    "                      id2word=dictionary, \n",
    "                      num_topics=2, \n",
    "                      passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.325*\"topic\" + 0.325*\"probabilistic\" + 0.324*\"model\" + 0.013*\"famous\" + 0.013*\"fashion\"'),\n",
       " (1,\n",
       "  '0.326*\"model\" + 0.323*\"fashion\" + 0.323*\"famous\" + 0.014*\"probabilistic\" + 0.014*\"topic\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el4231404203324471843128791654\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el4231404203324471843128791654_data = {\"mdsDat\": {\"x\": [0.1767163127660751, -0.1767163127660751], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [52.76336441766993, 47.236635582330074]}, \"tinfo\": {\"Term\": [\"fashion\", \"famous\", \"topic\", \"probabilistic\", \"model\", \"topic\", \"probabilistic\", \"model\", \"famous\", \"fashion\", \"fashion\", \"famous\", \"model\", \"probabilistic\", \"topic\"], \"Freq\": [12.0, 12.0, 13.0, 13.0, 25.0, 13.369319461771573, 13.358989649661158, 13.349329521404018, 0.542810882761018, 0.5349746771138807, 11.891290258794973, 11.883176738460396, 12.025531741402007, 0.5276374134431068, 0.516941785027021], \"Total\": [12.0, 12.0, 13.0, 13.0, 25.0, 13.886261246798593, 13.886627063104264, 25.374861262806025, 12.425987621221413, 12.426264935908854, 12.426264935908854, 12.425987621221413, 25.374861262806025, 13.886627063104264, 13.886261246798593], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [5.0, 4.0, 3.0, 2.0, 1.0, -1.124400019645691, -1.1252000331878662, -1.1259000301361084, -4.328400135040283, -4.342899799346924, -1.1309000253677368, -1.131600022315979, -1.1196999549865723, -4.246099948883057, -4.266499996185303], \"loglift\": [5.0, 4.0, 3.0, 2.0, 1.0, 0.6014, 0.6006, -0.0029, -2.4914, -2.506, 0.706, 0.7053, 0.0033, -2.5203, -2.5407]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.08047650057949317, 0.9657180069539181, 0.08047470460011243, 0.9656964552013491, 0.5123180720225314, 0.4729089895592598, 0.9361524537905993, 0.07201172721466148, 0.9361771155642836, 0.07201362427417567], \"Term\": [\"famous\", \"famous\", \"fashion\", \"fashion\", \"model\", \"model\", \"probabilistic\", \"probabilistic\", \"topic\", \"topic\"]}, \"R\": 5, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el4231404203324471843128791654\", ldavis_el4231404203324471843128791654_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el4231404203324471843128791654\", ldavis_el4231404203324471843128791654_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el4231404203324471843128791654\", ldavis_el4231404203324471843128791654_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x    y  topics  cluster       Freq\n",
       "topic                                           \n",
       "0      0.176716  0.0  1       1        52.763364\n",
       "1     -0.176716  0.0  2       1        47.236636, topic_info=            Term       Freq      Total Category  logprob  loglift\n",
       "1  fashion        12.000000  12.000000  Default  5.0000   5.0000 \n",
       "0  famous         12.000000  12.000000  Default  4.0000   4.0000 \n",
       "4  topic          13.000000  13.000000  Default  3.0000   3.0000 \n",
       "3  probabilistic  13.000000  13.000000  Default  2.0000   2.0000 \n",
       "2  model          25.000000  25.000000  Default  1.0000   1.0000 \n",
       "4  topic          13.369319  13.886261  Topic1  -1.1244   0.6014 \n",
       "3  probabilistic  13.358990  13.886627  Topic1  -1.1252   0.6006 \n",
       "2  model          13.349330  25.374861  Topic1  -1.1259  -0.0029 \n",
       "0  famous         0.542811   12.425988  Topic1  -4.3284  -2.4914 \n",
       "1  fashion        0.534975   12.426265  Topic1  -4.3429  -2.5060 \n",
       "1  fashion        11.891290  12.426265  Topic2  -1.1309   0.7060 \n",
       "0  famous         11.883177  12.425988  Topic2  -1.1316   0.7053 \n",
       "2  model          12.025532  25.374861  Topic2  -1.1197   0.0033 \n",
       "3  probabilistic  0.527637   13.886627  Topic2  -4.2461  -2.5203 \n",
       "4  topic          0.516942   13.886261  Topic2  -4.2665  -2.5407 , token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "0     1      0.080477  famous       \n",
       "0     2      0.965718  famous       \n",
       "1     1      0.080475  fashion      \n",
       "1     2      0.965696  fashion      \n",
       "2     1      0.512318  model        \n",
       "2     2      0.472909  model        \n",
       "3     1      0.936152  probabilistic\n",
       "3     2      0.072012  probabilistic\n",
       "4     1      0.936177  topic        \n",
       "4     2      0.072014  topic        , R=5, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda, doc_term_matrix, dictionary, sort_topics=False)\n",
    "vis\n",
    "\n",
    "# END SOLUTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tips when you build an LDA model on a large corpus \n",
    "\n",
    "- Preprocessing is crucial!! \n",
    "    - Tokenize, remove punctuation, convert text to lower case\n",
    "    - Discard words with length < threshold or word frequency < threshold        \n",
    "    - Stoplist: Remove most commonly used words in English \n",
    "    - Lemmatization: Consider the root form of the word. \n",
    "    - Restrict to specific part of speech\n",
    "        * Only consider nouns, verbs, and adjectives"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
