{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs\n",
    "### W04D1 Programming in Python for DS\n",
    "\n",
    "Instructor: Socorro Dominguez  \n",
    "June 14, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "1. What is and why do we do dimensionality reduction?\n",
    "\n",
    "\n",
    "    * Variable Selection Techniques\n",
    "        - Filter Methods\n",
    "        - Wrapper Methods\n",
    "       \n",
    "    * Dimensionality Reduction\n",
    "        - Principal Components Analysis (PCA)\n",
    "        - Linear Discriminant Analysis (LDA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What do you think is dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Imagine that you have a dataset of patients with two features: \n",
    "- Height \n",
    "- Weight\n",
    "\n",
    "Can you plot it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we add a third variable? Can you still plot it?\n",
    "- Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A fourth variable? Can you still plot it?\n",
    "- Net worth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dimensionality reduction** is:\n",
    "\n",
    "- Reducing the number of features in a dataset\n",
    "- E.g., 1000 rows by 20 columns (features) to 1000 rows by 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Helps our machine learning algorithms perform better\n",
    "- Improves run-time of our algorithms\n",
    "- Storing and using less data (memory)\n",
    "- For visualization\n",
    "\n",
    "**The best solution is the most parsimonious model with acceptable accuracy (or other metric).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When do we do dimensionality reduction?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before visualization: \n",
    "    - The human visual system is the most powerful perceptual system in the known universe... But, it only works in up to 3 dimensions.\n",
    "\n",
    "\n",
    "\n",
    "- To improve the performance of our baseline model  \n",
    "Example:\n",
    "   - Built a satellite imagery object detection model\n",
    "   - Satellite imagery has 12 channels compared to normal images which have 3\n",
    "   - We kept getting ~83-85% accuracy results until we finally tried PCA\n",
    "   - We originally had 12 dimensions; feature engineered an additional 66 features for a total of 78\n",
    "   - We reduced the 78 down to 3 and got 95% accuracy\n",
    "    \n",
    "**You don't know until you try.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do you think we can reduce the number of **features** from our dataset?\n",
    "\n",
    "Actually, we have two ways:\n",
    "\n",
    "- **Feature Selection**: selecting and excluding given features without changing them.\n",
    "- **Dimensionality Reduction**: transform features into a lower dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "The easiest way to reduce features is to keep the most important features and \"eliminating\" the others.\n",
    "\n",
    "The resulting feature set will still be interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection Techniques\n",
    "\n",
    "**Filter methods**  \n",
    "\n",
    "- Measure relevance of feature by correlation with dependent variable (target).\n",
    "- If feature is correlated with target, keep. Otherwise, discard\n",
    "- Applied before training ML model\n",
    "\n",
    "- Advantages: \n",
    "    - Fast, no training involved\n",
    "- Disadvantages: \n",
    "    - Ignores feature combinations\n",
    "    - Keeps redundant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![imgs](imgs/Filter_Methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Wrapper methods**  \n",
    "- Train ML model with different subsets of feature\n",
    "- If feature improves performance, add/keep it. Otherwise, ignore/remove it.\n",
    "- Applied during training ML model\n",
    "- Advantages:\n",
    "    - Evaluates features in context of others\n",
    "    - Performance-driven\n",
    "- Disadvantages:\n",
    "    - Slow, retrain model several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![imgs](imgs/Wrapper_Methods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward selection wrapper method\n",
    "\n",
    "1. SelectedFeatures = [ ]\n",
    "2. Find F in (AllFeatures - SelectedFeatures) that, if added to SelectedFeatures, best improves model performance\n",
    "3. If adding F improved performance more than some threshold, permanently add it to SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward elimination wrapper method\n",
    "\n",
    "1. SelectedFeatures = AllFeatures\n",
    "2. Find F in SelectedFeatures that, if removed from SelectedFeatures, decreases model performance the least\n",
    "3. If removing F decreased performance less than some threshold, permanently remove it from SelectedFeatures and go back to (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "1. Decide $k$, the number of features to select. \n",
    "* Use a model (usually a linear model) to assign weights to features.\n",
    "    - The weights of important features have higher absolute value.\n",
    "* Rank the features based on the absolute value of weights.\n",
    "* Drop the least useful feature.\n",
    "* Try steps 2-4 again until desired number of features is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection - Wrapper Methods Tips\n",
    "- Look for implementations, `sklearn` has a `rfe` implementations, for example\n",
    "- It's not possible to tell which method will work better until you try\n",
    "- Different variable selection algorithms may give you a different answers\n",
    "- Different machine learning algorithms with the same variable selection method may give you given answers\n",
    "- Over this process, you'll find out what features tend to get eliminated and which features tend to be kept (hopefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What if....????\n",
    "\n",
    "Instead of eliminating some of our features, we transformed features to keep as much information as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**The goal is to preserve as much of the important data as possible.**\n",
    "\n",
    "Two well-known techniques amongst many others that we'll cover today:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "- Consider a data matrix $X$ with $n$ rows and $d$ columns\n",
    "\n",
    "\n",
    "- We want a new data matrix $Z$ with $n$ rows and $k\\leq d$ columns\n",
    "\n",
    "$$\n",
    "\\textbf{X} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &  &   & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    \\textbf{X}_{1}    & \\textbf{X}_{2}    & \\ldots & \\ldots & \\ldots & \\textbf{X}_{d}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} & \\rule[-1ex]{0.5pt}{2.5ex} &   &   &  & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{d columns (wider)}}\\\\\n",
    "\\textbf{Z} = \n",
    "\\underbrace{\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex} &         & \\rule[-1ex]{0.5pt}{2.5ex} \\\\\n",
    "    {\\textbf{Z}}_{1}    & \\ldots & \\textbf{Z}_{k}    \\\\\n",
    "    \\rule[-1ex]{0.5pt}{2.5ex}  &        & \\rule[-1ex]{0.5pt}{2.5ex} \n",
    "  \\end{array}\n",
    "\\right]}_{\\text{k columns (narrower)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's imagine we have a dataset with two features:\n",
    "- height\n",
    "- weight\n",
    "\n",
    "And we plot them this way:  \n",
    "![img1](imgs/PCA01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our task is to project this data into a smaller dimension: a line.\n",
    "\n",
    "First, for all observations, we calculate the average measure for `Height` and then, the average measure for `Weight`\n",
    "\n",
    "![img1](imgs/PCA01a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's shift the data in such a manner that the center of the data, becomes the origin.\n",
    "\n",
    "![img1](imgs/PCA01b.png)\n",
    "\n",
    "** Data points are still related among themselves the same way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try now to fit a random line that captures most of our data points information. This line MUST pass through the origin.\n",
    "\n",
    "![img1](imgs/PCA01c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we get the best line?\n",
    "\n",
    "- PCA projects the data on the line.\n",
    "- PCA finds the line that maximizes the distances from the projected points to the origin.\n",
    "    - This is the same as minimizing the distance between the line and the data observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PCA will measure the distance from the origin to each projected observation. \n",
    "\n",
    "If we only had 5 observations, it would only have 5 distances:\n",
    "\n",
    "$d_1 + d_2 + d_3 + d_4 + d_5$\n",
    "\n",
    "and then, squares them up:\n",
    "\n",
    "${d_1}^2 + {d_2}^2 + {d_3}^2 + {d_4}^2 + {d_5}^2 = SS(distances)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img1](imgs/PCA01d.png)\n",
    "\n",
    "We do this until we get the largest $SS(distances)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This new line is called **Principal Component 1 (PC1)**\n",
    "\n",
    "**SS(distances) for PC1** is called the **eigenvalue for PC1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say that our **PC1** has a slope of 0.5\n",
    "\n",
    "That is, for every 2 unit increase in **height**, we increase 1 unit in **weight**.\n",
    "\n",
    "Here, we can say then that for PC1, **height** is more important than **weight**.\n",
    "\n",
    "**Data is more spread out on the height axis**\n",
    "\n",
    "**PC1** ends up being a **Linear Combination** of:   \n",
    "> $PC1 = 2*Height + 1*Weight$\n",
    "\n",
    "When we make the vector have a measure of one, by normalizing it, we end up having the **eigenvector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With 2 features, it is easy to find **PC2**, it has to be the line that also passes through the origin and that is ortogonal to **PC1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](imgs/PCA02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we only have 2 dimensions, PC2 must be:\n",
    "> $PC2 = -1*Height + 2*Weight$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For our final plot, we rotate everything so that PC1 and PC2 are horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![img](imgs/PCA03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you had an extra dimension, you would still need to do some extra optimization. Make sure you find the best line and PC3 would just be finding the extra line that is ortogonal to both PC1 and PC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Variation\n",
    "\n",
    "We will aid ourselves with a `Scree Plot` to measure variation. \n",
    "\n",
    "$Variation(PC1) = \\frac{SS(distances_{PC1})}{n-1}$  \n",
    "$Variation(PC2) = \\frac{SS(distances_{PC2})}{n-1}$  \n",
    "...  \n",
    "$Variation(PCn) = \\frac{SS(distances_{PCn})}{n-1}$  \n",
    "\n",
    "![img](imgs/scree_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a **Scree Plot** you might determine that you only need the first 2 or 3 PCs rather than the complete set of PCs for a better model.\n",
    "\n",
    "Remember, the max number of PCs that you have:  \n",
    "a) number of features  \n",
    "b) number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "- Similar to PCA: Projecting onto smaller number of dimensions\n",
    "- Different from PCA: Uses the `y` or class label to help us decide what to select\n",
    "- Can only use for classification (remember, when `y` is discrete)\n",
    "\n",
    "<img src='imgs/pca-v-lda.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing the Iris dataset\n",
    "\n",
    "<img src='imgs/iris-dataset.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](imgs/LDA01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDA: concept of interclass variance\n",
    "- Which features better delineates the classes?\n",
    "\n",
    "<img src='imgs/lda-iris.png' width=800>\n",
    "\n",
    "**We are trying to find components that minimize the intra-class variance and maximizes the inter-class variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![img](imgs/LDA02.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
